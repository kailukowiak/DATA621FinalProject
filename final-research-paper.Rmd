---
title: Determining Factors Influencing House Sale Prices
author: | 
        | **Kile Gilde, Jaan Bernberg, Kai Lukowiak,**
        | **Michael Muller, Ilya Kats**
        | DATA 621, Master of Science in Data Science,
        | City University of New York

output: 
  pdf_document:
    df_print: kable
    toc: no
    fig_caption: yes

nocite: | 
  @wickham_ggplot2_2009, @RCoreTeam, @Chang2015
  
#abstract: "This document provides an introduction to R Markdown, argues for its..."
#keywords: "pandoc, r markdown, knitr"
bibliography: bibliography.bib
---


```{r options_pkgs, echo=F, warning=F, message=F, results=F}
knitr::opts_chunk$set(error = F, message = F, # tidy = T,
                      cache = T, warning = T, 
                      results = 'hide', # suppress code output
                      echo = F,         # suppress code
                      fig.show = 'hide' # suppress plots
                      )

library(RCurl)
library(knitr)
library(knitcitations)
library(pander)
```

# Abstract

TBD: Since it summarizes the work, it will be written at the end. 250 words or less summarizing the problem, methodology, and major
outcomes.

# Key Words

house prices, regression, linear models, assessed value

# Introduction

This project stems out of the Business Analytics and Data Mining class in the Master of Science in Data Science program at CUNY. This paper is the result of the final class group project in applying regression methods to real-world data. Our team chose housing data because it promissed to be an interesting and useful subject. In addition, this research is based on a well studied data set which makes it an excellent educational resource allowing our team to study various approaches.

The data set was prepared by Dean De Cock in an effort to create a real-world data set to test and practice regression methods [@DeCock]. It describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. Ames, Iowa was founded in 1864 as a station stop. It has the population of about 60,000 people and covers about 24.27 sq mi. It was ranked ninth on the *Best Places to Live* list [@CNNMoney].

The data came directly from the Assessor's Office in the form of a data dump from their records system and it included information for calculation of assessed values in the city's assessment process. The data is recent and it covers the period of housing bubble collapse that led to the subprime mortgage crisis. 2008 saw one of the largest housing price drops in history.

Each of over 2,900 total observations in the data represent attributes of a residental property sold. For properties that exchanged ownership multiple times during the collection period (2006 through 2010), only the last sale is included in the data since it represents the most current value of the property. The attributes that make up the sale price of a house can seem daunting given a myriad of factors that can impact its value. There are about 80 variables included in the data set. Most variables describe physical attributes of the property. There is a variety of variable types - discrete, continous, categorical (both nominal and ordinal).

The data was originally published in the Journal of Statistics Education (Volume 19, Number 3). Data set was downloaded from Kaggle.com which gave us the ability to compare our results with results of other teams working with this data set [@Kaggle].

# Literature Review

Building regression models to predict house prices is not a new undertaking. Quite the opposite, a lot of research went into this area. There is a clear financial benefit to buyers, sellers and other parties in knowing which attributes influence final sale price. There is also a lot of data readily available with some cleanup work. Data is kept by local governments to be used in the assessment process for property taxes. There is a lot of data captured by realors when a property is listed on the market. Additionally, in large part thanks to information revolution, data is easily accessible via many aggregators such as MLS.

There are many attributes that factor into a house price. For example, environmental attributes can impact the price substantially. A garden facing water, a pleasant view whether it overlooks water or open space, attractive landscaping all increase house prices [@luttik]. Neighborhood attributes such as schools and public services also play a factor. 

Our data set deals mostly with physical characteristics of the house itself. Even here there is a lot of room for variation. For example, one study counted half-bathrooms as 0.1 out of belief that buyers do not value them as much as full bathrooms [@pardoe].

...

- 1 page
- Discuss how other researchers have addressed similar problems, what their achievements are, and what the advantage and drawbacks of each reviewed
approach are. 
- Explain how your investigation is similar or different to the state-of-theart.

# Methodology

- 2-3 pages
- Discuss high-level exploratory data analysis - how data was prepared
- Discuss high-level regression modeling
- Discuss high-level model building and model selection

### Data Description

The data set includes 2,919 observation and 79 indepedent variables. Out of those 36 are numeric, such as lot area or pool area in square feet, and 43 are categorical, such as garage type (attached to home, built-in, carport, etc.) or utilities (gas, sewer, both, etc.).

### Data Imputation

Original data set included no complete observations (*see table 1*). However, many `NA` values found in the data carry useable information. For example, `NA` in the `PoolQC` variable (pool quality) implies that the property has no pool. Often this logic carried across multiple variables - for example, `NA` in `GarageQual` (garage quality), `GarageCond` (garage condition) and `GarageType` variables all imply that the property has no garage. This type of missing values was replaced with a new category - *No Pool*, *No Garage* or similar. This work was accomplished using the `forcats` R package.

After this substitution the number of complete observations went up significantly to 2,861 or about 98% of all observations. There remained only 58 observations with true missing values (about 2% of the total observations). These observations contained 180 missing values in 32 variables. None of the variables contained a large number of missing values. The top one was `MasVnrType` with 24 observations containing `NA` (0.8% of all observations). None of the variables were close to the 5% missing threshold that would suggest that we should drop them from analysis.

Consider the pattern to the missing values. In addition to the quantity of missingness being important, why and how the values are missing can give us insight into whether we have a biased sample. There are three types of missing data [@faraway]: 1) Missing Completely at Random (MCAR), 2) Missing at Random (MAR), and 3) Missing Not at Random (MNAR). MCAR is when the probability of missingness is the same for all cases. This is the ideal type of missingness because we could delete these cases without incurring bias. MAR occurs when the probability of a value being missing depends upon a known mechanism. In this scenario, we could delete these observations and compensate by weighting by group membership. Finally, MNAR occurs when the values are missing because of an unknown variable. This is the type of missingness that is most likely to bias our sample. Faraway asserts that ascertaining the exact nature of the missingness is not possible and must be inferred. Figure 1 displays the combinations of missing values in the predictor variables. We may not have MCAR because we can see that the missingness is not more dispersed across all variables and  cases. Only 32 of the 79 predictors have a missing value, and we notice that the missingness occurs most often in some of the masonry, basement and garage variables. There is no indication that values are missing not at random and given the small number of missing values, we believe the bias, if any, will be limited.

There are four ways to deal with missing values [@Prabhakaran]:

- **Deleting the cases:** This is not a preferred method because one could introduce bias or the model could lose power from being based upon fewer cases.
- **Deleting the variables:** If the missingness is concentrated in a relatively small number of variables, then deleting the variables may be a good option. The downside to this approach is that we lose the opportunity to include the observed values in the model.
- **Imputation via mean, median and mode:** An expedient way to retain all of the cases and variables is to insert the mean or median for continuous variables or the mode for categorical or discrete variables. This approach may suffice for a small number of values, but has the potential to introduce bias in the form of decreasing the variance.
- **Prediction:** This more advanced approach involves using the other variables to predict the missing values.

For our data set we used multiple imputation by chained equations (MICE). The technique involves imputing multiple iterations of values in order to account for statistical uncertainty with standard errors [@azur]. Since it uses chained equations, MICE has the ability to impute both numerical and categorical variables. The ideal scenario to use MICE is when less than 5% of the values are missing and when values are missing at random. We used the `mice` R package with the `cart` (classification and regression trees) method. CART is one of the five `mice` methods that can impute both numerical and categorical variables. Figure 2 shows the density plots of the observed and imputed values. The imputed distributions have more variance and extremes than the observed distributions. If we were to run, multiple imputations, hopefully we would begin to see more convergence between the imputed and observed values.

```{r NA Review}
naVars <- read.csv(paste0("https://raw.githubusercontent.com/kaiserxc/DATA621FinalProject/",
                          "master/report_files/table1_na_vars.csv"))
naVars <- naVars[,2:4]
colnames(naVars) <- c("Variable", "No of NAs", "Percent of Total Obs")
pander(naVars, keep.trailing.zeros=TRUE, 
       caption="Number of NA values in original data.",
       justify=c("left", "right", "right"))
```

### Additional Data Preparation

All categorical variables were inspected and their order (or order of levels in R) was changed to match the most likely low-to-high order. These variables for the most part do not rely on the order of categories, so this step was not critical to modeling; however, it makes modeling output more readable and easier to interpret.

As is the case with most data sets, we found several values that were clearly typos and input errors. For instance, one observation had the year when garage was built listed as 2207. There were 6 negative values in age related variables (see data transformations below). Those were set to 0. 

### Data Transformation

Generally, it is more common to think about the age of the house than the year it was built. Each age related variable was stored in the data set in two related variables - year built and year sold. Rather than trying to work with original variables we have converted them to a single *age* variable. For house age the value was $YrSold - YearBuilt$. Similarly the age of garage and remodeling was added to the data set. Original variables were dropped from analysis.

Because we are not dealing with a time series data set, we have converted `YrSold` and `MoSold` variables from numeric to nominal. It is important to catch seasonality, but does not make sense to regress on these variables as continous variables.

### Modeling

Since we are dealing with trying to predict a continous variable, house sale price, we relied on building and optimizing general linear model.

# Experimentation and Results

- 4-5 pages
- Key figures and tables may be included here
- Additional figures and tables should be added to appendices
- Discuss data prepatation details not mentioned under Methodology
- Discuss model building and selection
- Discuss model validation
- Discuss results of statistical analysis
- Describe final model (coefficients, interpretation)
- Discuss upload of results to Kaggle

# Discussion

- 1-2 pages
- Discuss limitations
- Discuss areas for future work
- Discuss detailed findings
- May be combined with Conclusion section below

# Conclusion

- 1 paragraph
- Quick summary of findings

```{r Summary Stats Numeric}
stat_tbl <- read.csv(paste0("https://raw.githubusercontent.com/kaiserxc/DATA621FinalProject/",
                            "master/report_files/table2_stats.csv"))
colnames(stat_tbl) <- c("Variable", "Count", "Mean", "SD", "Median", "Min", "Max", "Kurtosis")
pander(stat_tbl, caption="Descriptive statistics.",
       digits=4, emphasize.rownames=FALSE,
       justify=c("left", "right", "right", "right", "right", "right", "right", "right"))
```

\newpage
# Appendix A. Figures

![](https://raw.githubusercontent.com/kaiserxc/DATA621FinalProject/master/report_files/fig1_na_dist.png)
\begin{center}
Figure 1. Missing values.
\end{center}

![](https://raw.githubusercontent.com/kaiserxc/DATA621FinalProject/master/report_files/fig2_imputation.png)
\begin{center}
Figure 2. Density plots of observed (blue) and imputed (red) values.
\end{center}

```{r Plots, ref.label=c("Figure1", "Figure2"), echo=F, fig.show="asis"}
```

\newpage
# Appendix B. Tables

```{r Table1, ref.label="NA Review", echo=F, eval=T, results=T}
```

\newpage
```{r Table2, ref.label="Summary Stats Numeric", echo=F, eval=T, results=T}
```

\newpage
# Appendix C. R Code

```{r Code, echo=T, eval=F}

# TBD

```

\newpage
# References

